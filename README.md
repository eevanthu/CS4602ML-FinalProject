# CS4602ML-FinalProject: TSMC (2330) Stock Movement Prediction
# ğŸ“ˆ Stacking Ensemble Architecture for Financial Forecasting

This project implements a robust machine learning pipeline to predict the price movement (Up/Down) of TSMC (2330). By leveraging a multi-model **Stacking Ensemble** approach, we combine the strengths of distance-based algorithms, tree-based models, and deep learning architectures to capture complex market dynamics.

---

## ğŸšª Quick Navigation (å‚³é€é–€)

| Resource | Description | Link |
| :--- | :--- | :--- |
| **ğŸŒ Website Gallery** | Curated financial data sources & research sites | [ğŸ”— View Sites](./website.md) |
| **ğŸ“ Data Repository** | Raw and processed datasets for 2330 forecasting | [ğŸ”— Explore Data](./data/) |
| **ğŸ“ CheckPoints** | Project milestones and phase-wise reports | [ğŸ”— CheckPoints](./checkpoint/) |
| **ğŸ“Š Result Hub** | Exported prediction probabilities (Meta-features) | [ğŸ”— View Results](./results/) |

---

## ğŸ—‚ï¸ Project Structure

The repository is organized into four core functional domains:

```text
.
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ backtest/            # Trading strategy & performance evaluation
â”‚   â”‚   â”œâ”€â”€ backtest6.py
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ models/              # Individual base learners
â”‚   â”‚   â”œâ”€â”€ KNN/             # K-Nearest Neighbors (K-Sweep & Cluster-Augmented)
â”‚   â”‚   â”œâ”€â”€ LSTM/            # Long Short-Term Memory (Deep Learning)
â”‚   â”‚   â”œâ”€â”€ RandomForest/    # Ensemble Tree-based model
â”‚   â”‚   â”œâ”€â”€ XGBoost/         # Gradient Boosting Machine
â”‚   â”‚   â”œâ”€â”€ NN/              # Multi-layer Perceptron / Neural Network
â”‚   â”‚   â”œâ”€â”€ NaiveBayes/      # Probabilistic Classifier
â”‚   â”‚   â””â”€â”€ (Each model contains its own README.md & requirements.txt)
â”‚   â”œâ”€â”€ preprocess/          # Data cleaning & feature engineering pipeline
â”‚   â”‚   â”œâ”€â”€ data_download.ipynb
â”‚   â”‚   â”œâ”€â”€ preprocess.ipynb
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ stack/               # Meta-learner for final ensemble prediction
â”‚       â”œâ”€â”€ stacking.ipynb
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/           # Cleaned features (train.csv, test.csv)
â”‚   â””â”€â”€ raw/                 # Source data (2330, ADR, DJI, SOX, etc.)
â””â”€â”€ results/                 # Exported prediction probabilities (Meta-features)
    â”œâ”€â”€ KNN/                 # KNN_prediction_all.csv, etc.
    â””â”€â”€ LSTM/                # LSTM_prediction_2025.csv, etc.       
```
## ğŸ“‚ Data Pipeline & Workflow

The project follows a modular "data-first" architecture designed for high extensibility:

1.  **Data Acquisition (`data_download.ipynb`)**: Retrieves raw signals including TSMC (2330) prices via the `twstock` API, along with U.S. market indices (DJI, NASDAQ, SOX, SPX) and TSMC ADR data.
2.  **Core Preprocessing (`preprocess.ipynb`)**: Cleans raw data and generates the standardized `train.csv` (2010â€“2024) and `test.csv` (2025) files, which are stored in `data/processed/`.
3.  **Model-Specific Engineering**: Base learners consume these core files and apply specialized processing. For instance, the **KNN model** expands the feature pool into a **117-dimensional** space using trend discretization and lag features to capture market momentum.
4.  **Result Persistence (`results/`)**: Every base model exports its predicted "upward probabilities" into specific subdirectories (e.g., `results/KNN/KNN_prediction_all.csv`).
5.  **Stacking Execution (`stacking.ipynb`)**: The script retrieves these persistent meta-features to train the meta-learner (XGBoost) for the final 2025 market direction forecast.

---

## ğŸš€ Key Technical Features

### 1. Advanced Preprocessing Principles
* **Temporal Alignment**: Corrects for U.S./Taiwan market time zone differences (Shift-1) to ensure **Zero Data Leakage**, preventing the model from "reading the answer" from same-day U.S. closes.
* **Logical Imputation**: Employs a `fill_na_safely` strategy where binary signals are treated as "no event" (0) and historical lags as "data absence" (-1) to maintain physical data integrity.
* **Relative Scaling**: Shifts from absolute price values to relative percentage changes to ensure models generalize to high-price environments not seen in historical training data.

### 2. Heterogeneous Base Learners (The "Zoo")
* **Spatial & Regime-Aware (KNN)**: Uses PCA for dimensionality reduction and K-Means clustering to identify "Market Regimes," allowing the model to search for neighbors within structurally similar historical segments.
* **Temporal & Sequential (LSTM/NN)**: Employs sliding windows (30â€“32 days) to capture long-term temporal dependencies and price inertia.
* **Tree-Based Interactions (XGBoost/Random Forest)**: Models complex non-linear feature interactions while using bagging and shallow depths to mitigate overfitting.
* **Probabilistic Baseline (Naive Bayes)**: Calculates conditional probabilities based on feature distributions to provide an efficient probabilistic baseline.

### 3. Stacking & Backtesting Strategy
* **Rolling Forecast Configuration**: Base learners generate predictions for the 2020â€“2024 period, which serve as the out-of-sample training input for the meta-learner.
* **Meta-Learner Fusion**: A shallow XGBoost classifier (max_depth 1â€“3) acts as the meta-learner, learning the optimal weighting for each base model to achieve higher accuracy than any single base learner.
* **Confidence-Based Trading**: The backtesting engine utilizes a "linear ratio" strategy, scaling the number of lots traded (up to 10) based on the modelâ€™s confidence score.

---

## ğŸ› ï¸ Execution Order

1.  **Prepare Data**: Run `code/preprocess/preprocess.ipynb` to generate the processed CSV files.
2.  **Generate Meta-features**: Execute individual base learner notebooks in `code/models/` to populate the `results/` folder.
3.  **Perform Stacking**: Run `code/stack/stacking.ipynb` to synthesize predictions.
4.  **Backtest**: Run `code/backtest/backtest6.py` to evaluate financial performance.

---

## ğŸ› ï¸ Getting Started

### Installation
Each module is designed with self-contained dependencies. It is recommended to use a virtual environment:

```bash
# 1. Clone the repository
git clone [https://github.com/your-repo/CS4602ML-FinalProject.git](https://github.com/your-repo/CS4602ML-FinalProject.git)

# 2. Setup environment for a specific module (e.g., KNN)
cd code/models/KNN
pip install -r requirements.txt
```

### ğŸ“ˆ Performance Summary
* Detailed results and evaluation metrics are updated per project milestone in the `CheckPoint` folder.
