{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf8ecd8-4a9e-44d5-b4db-7e34905fe4e4",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Stock Movement Prediction Model: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### 1. Model Overview\n",
    "This program implements a K-Nearest Neighbors (KNN) machine learning model, aiming to predict the stock price direction (Up/Down) of TSMC (2330) by leveraging multi-dimensional technical indicators, global macro data, and related market information.\n",
    "- **KNN**: A distance-based classification algorithm with a strong capability to capture non-linear relationships between financial features.\n",
    "\n",
    "### 2. Feature Engineering\n",
    "The primary features utilized in this model include:\n",
    "- **Price and Volume Indicators**: Open, High, Low, Close, Volume, Adj Close.\n",
    "- **Technical Indicators**: Moving Averages (MA), Relative Strength Index (RSI), and MACD series indicators.\n",
    "- **Market Interdependency**: TSMC ADR (ADR_Close), Dow Jones Industrial Average (DJI_Close), and Volatility Index (VIX_Close).\n",
    "- **Preprocessing**: `StandardScaler` standardization is applied to the KNN model to eliminate feature scale differences and ensure distance-based accuracy.\n",
    "\n",
    "### 3. Requirements\n",
    "Ensure the following packages are installed before execution:\n",
    "- `pandas`, `numpy`, `scikit-learn`, `matplotlib`\n",
    "\n",
    "### 4. File Paths\n",
    "- **Input**: `../../../data/processed/train.csv` & `test.csv`\n",
    "- **Output**: `../../../data/results/KNN_pred.csv` (The prediction results will be exported as meta-features for the subsequent Stacking ensemble layer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8af98-c1f4-4b47-9f02-6654eca1ca93",
   "metadata": {},
   "source": [
    "# ðŸ“Š Phase 1: Data Acquisition & Environment Setup\n",
    "\n",
    "This project aims to construct a robust movement prediction model for TSMC (2330) stock. Before commencing feature engineering and model training, we first perform data loading and verify the underlying data structure.\n",
    "\n",
    "### 1. Data Sources\n",
    "We have integrated multi-dimensional financial data spanning from 2010 to 2025, totaling sixteen years of trading records:\n",
    "* **Taiwan Market Trading Data**: Includes price and volume information (Open, Close, High, Low, Volume) for TSMC (2330).\n",
    "* **Technical Indicators**: The raw dataset contains key quantitative indicators such as Moving Averages (MA), RSI, MACD, and K/D.\n",
    "* **U.S. Market Interdependency**: Integration of major U.S. indices, including NASDAQ, S&P 500 (SPX), Dow Jones Industrial Average (DJI), and the Philadelphia Semiconductor Index (SOX).\n",
    "* **External Sentiment Indicators**: TSMC ADR price movements and the VIX Volatility Index.\n",
    "\n",
    "### 2. Dataset Splitting Principles\n",
    "* **Training Set**: Utilizes historical data from 2010 to 2024 to build a stable market feature model.\n",
    "* **Test Set**: Uses 2025 data as an Out-of-Sample test to verify the model's generalization ability on unseen data.\n",
    "\n",
    "### 3. Objectives\n",
    "The primary objective of this phase is to establish verified data retrieval paths and perform standardization on time-series fields. This ensures data quality and consistency, laying the critical foundation for subsequent Feature Enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33bde70-d105-47be-8a6b-2139858393f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e69ea2d-c6d3-46d0-9fc2-c78522776c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"../../../data/processed/train.csv\")\n",
    "test_raw  = pd.read_csv(\"../../../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4bda4d-baf8-47ad-9184-7f863bc237b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æžœ Date æ˜¯å­—ä¸²ï¼Œè½‰æˆ datetime\n",
    "if 'Date' in train_raw.columns:\n",
    "    train_raw['Date'] = pd.to_datetime(train_raw['Date'])\n",
    "    test_raw['Date']  = pd.to_datetime(test_raw['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05cc7c8a-b0d1-4c73-9cd4-399e5d98f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Capacity', 'Turnover', 'Open', 'High', 'Low', 'Close', 'Change', 'Transaction', 'MA5', 'MA10', 'MA5_Capacity', 'BR5', 'BR10', 'High-Low', 'Open-Close', 'EMA5', 'EMA10', 'K', 'D', 'MACD', 'MACD_signal', 'MACD_hist', 'RSI14', 'STD20', 'ADX', 'RSI', 'DJI', 'NASDAQ', 'SOX', 'SPX', 'ADR', 'twclose', 'twopen', 'Movement', 'rate']\n"
     ]
    }
   ],
   "source": [
    "print(train_raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cbb30-77d4-41c8-bbc3-b41ac968a1fc",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Phase 2: Data Preprocessing & Feature Engineering\n",
    "\n",
    "This phase transforms raw financial data into features that machine learning models can interpret. We retain fundamental price-volume information and expand the analysis dimensions through a custom `enhance_features` function.\n",
    "\n",
    "### 1. Core Feature Categories\n",
    "* **Market Fundamentals**: Includes raw price and volume data such as Open, High, Low, Close, and Volume.\n",
    "* **Technical Analysis Indicators**: \n",
    "    * Trend Indicators: MA5, MA10, EMA, and MACD.\n",
    "    * Momentum Indicators: RSI, K, D, and ADX.\n",
    "    * Volatility Indicators: STD20 and Bollinger Bands.\n",
    "* **Global Market Interdependency**: Integration of major U.S. indices (DJI, NASDAQ, SOX, SPX) and TSMC ADR price movements.\n",
    "* **Custom Enhanced Features**: \n",
    "    * **Lags**: Captures time-series inertia by considering price and momentum changes over the past 1 to 10 days.\n",
    "    * **Pattern Analysis**: Mathematical representation of candlestick body size, shadow ratios, and specialized price formations.\n",
    "    * **Continuity Indicators**: Tracks consecutive up/down days to detect extreme market sentiment and potential reversals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9114c25-fe44-45b6-be77-7d52f52d2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ç‰¹å¾µå·¥ç¨‹å¢žå¼·å‡½å¼ï¼š\n",
    "    å°‡åŽŸå§‹è‚¡åƒ¹èˆ‡æŒ‡æ¨™æ•¸æ“šè½‰æ›ç‚ºæ›´å…·é æ¸¬åŠ›çš„ç‰¹å¾µï¼ˆåŒ…å«å‹•é‡ã€åž‹æ…‹ã€å‡ç·šäº¤å‰ã€ç¾Žè‚¡è¯å‹•ç­‰ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ç¢ºä¿æŒ‰æ—¥æœŸæŽ’åºï¼šæ™‚é–“åºåˆ—æ¨¡åž‹å¿…é ˆç¢ºä¿éŽåŽ»çš„è³‡æ–™ä¸æœƒåƒè€ƒåˆ°æœªä¾†ï¼ˆé˜²æ­¢è³‡æ–™æ´©æ¼ï¼‰\n",
    "    if 'Date' in df.columns:\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # ============= 1. åŸºç¤Žåƒ¹æ ¼èˆ‡æŠ€è¡“æŒ‡æ¨™è¡ç”Ÿç‰¹å¾µ =============\n",
    "    # è¨ˆç®—ç•¶æ—¥æ¼²è·Œå¹…èˆ‡éœ‡å¹…\n",
    "    df['Price_momentum'] = df['Close'] / df['Open'] - 1\n",
    "    df['High_Low_range'] = (df['High'] - df['Low']) / df['Close']\n",
    "\n",
    "    # å‡ç·šäº¤å‰ç‰¹å¾µï¼šåˆ¤æ–·çŸ­æœŸè¶¨å‹¢æ˜¯å¦å¼·æ–¼ä¸­æœŸ\n",
    "    df['MA5_MA10_cross']   = (df['MA5'] > df['MA10']).astype(int)\n",
    "    df['Close_above_MA5']  = (df['Close'] > df['MA5']).astype(int)\n",
    "    df['Close_above_MA10'] = (df['Close'] > df['MA10']).astype(int)\n",
    "\n",
    "    # RSI å¼·å¼±åˆ¤æ–·ï¼šå®šç¾©è¶…è²·(>70)èˆ‡è¶…è·Œ(<30)å€é–“\n",
    "    df['RSI_oversold']   = (df['RSI'] < 30).astype(int)\n",
    "    df['RSI_overbought'] = (df['RSI'] > 70).astype(int)\n",
    "\n",
    "    # MACD è¶¨å‹¢èˆ‡é»ƒé‡‘äº¤å‰åˆ¤æ–·\n",
    "    df['MACD_above_signal'] = (df['MACD'] > df['MACD_signal']).astype(int)\n",
    "    df['MACD_golden_cross'] = (\n",
    "        (df['MACD'] > df['MACD_signal']) &\n",
    "        (df['MACD'].shift(1) <= df['MACD_signal'].shift(1))\n",
    "    ).astype(int)\n",
    "    df['MACD_positive'] = (df['MACD'] > 0).astype(int)\n",
    "\n",
    "    # KD æŒ‡æ¨™äº¤å‰èˆ‡å€é–“åˆ¤æ–· (åˆ¤æ–·çŸ­æœŸè½‰æŠ˜)\n",
    "    df['K_above_D'] = (df['K'] > df['D']).astype(int)\n",
    "    df['K_D_golden_cross'] = (\n",
    "        (df['K'] > df['D']) & (df['K'].shift(1) <= df['D'].shift(1))\n",
    "    ).astype(int)\n",
    "    df['K_D_dead_cross'] = (\n",
    "        (df['K'] < df['D']) & (df['K'].shift(1) >= df['D'].shift(1))\n",
    "    ).astype(int)\n",
    "    df['KD_oversold']   = ((df['K'] < 20) & (df['D'] < 20)).astype(int)\n",
    "    df['KD_overbought'] = ((df['K'] > 80) & (df['D'] > 80)).astype(int)\n",
    "\n",
    "    # ç¾Žè‚¡æŒ‡æ•¸è¯å‹•ï¼šè¨ˆç®—é“ç“Š(DJI)ã€æ¨™æ™®500(SPX)ã€é‚£æ–¯é”å…‹(NASDAQ)çš„ç•¶æ—¥è®Šå‹•çŽ‡\n",
    "    df['DJI_change']    = df['DJI'].pct_change()\n",
    "    df['SPX_change']    = df['SPX'].pct_change()\n",
    "    df['NASDAQ_change'] = df['NASDAQ'].pct_change()\n",
    "\n",
    "    # ç›¸å°å¼·å¼±ç‰¹å¾µï¼šå€‹è‚¡ç›¸å°æ–¼å¤§ç›¤(twclose)çš„è¶…é¡å ±é…¬\n",
    "    df['Stock_vs_TW'] = df['Close'].pct_change() - df['twclose'].pct_change()\n",
    "\n",
    "    # æ»¯å¾Œé …ç‰¹å¾µ (Lags)ï¼šæ•æ‰æ­·å²æ•¸æ“šçš„æ…£æ€§ï¼ˆè€ƒæ…®éŽåŽ» 1~10 å¤©çš„æ¼²è·Œã€æ¼²å¹…èˆ‡ RSIï¼‰\n",
    "    for lag in [1, 2, 3, 4, 5, 6, 7, 10]:\n",
    "        df[f'Movement_lag{lag}'] = df['Movement'].shift(lag)\n",
    "        df[f'Change_lag{lag}']   = df['Change'].shift(lag)\n",
    "        df[f'RSI_lag{lag}']      = df['RSI'].shift(lag)\n",
    "        df[f'Return{lag}']       = df['Close'].pct_change(lag)\n",
    "\n",
    "    # æ³¢å‹•åº¦ç‰¹å¾µï¼šè¨ˆç®— 5 æ—¥çŸ­ç·šèˆ‡ 20 æ—¥é•·ç·šçš„æ³¢å‹•æ¯”ä¾‹\n",
    "    df['Volatility20']     = df['STD20'] \n",
    "    df['Volatility5']      = df['Close'].rolling(5).std()\n",
    "    df['Volatility_ratio'] = df['Volatility5'] / (df['Volatility20'] + 1e-8)\n",
    "\n",
    "    # æ»¾å‹•å¹³å‡ç‰¹å¾µï¼šå¹³æ»‘åŒ–è¿‘ 5 æ—¥çš„è¡¨ç¾\n",
    "    df['Change_mean_5d'] = df['Change'].rolling(5).mean()\n",
    "    df['Change_std_5d']  = df['Change'].rolling(5).std()\n",
    "    df['RSI_mean_5d']    = df['RSI'].rolling(5).mean()\n",
    "\n",
    "    # è¨ˆç®—é€£çºŒä¸Šæ¼²/ä¸‹è·Œå¤©æ•¸ï¼šæ•æ‰æ¥µç«¯æƒ…ç·’èˆ‡åè½‰å¯èƒ½\n",
    "    up_mask   = (df['Movement'] == 1).astype(int)\n",
    "    down_mask = (df['Movement'] == 0).astype(int)\n",
    "    df['Consecutive_up'] = up_mask.groupby((up_mask == 0).cumsum()).cumsum()\n",
    "    df['Consecutive_down'] = down_mask.groupby((down_mask == 0).cumsum()).cumsum()\n",
    "\n",
    "    # ============= 2. K æ£’å¯¦é«”èˆ‡å½±ç·šåˆ†æžï¼ˆåƒ¹æ ¼åž‹æ…‹ï¼‰ =============\n",
    "    price_range = (df['High'] - df['Low']).replace(0, np.nan)\n",
    "    # æ”¶ç›¤åƒ¹åœ¨ç•¶æ—¥æœ€é«˜ä½Žé»žçš„ä½ç½®ï¼šåˆ¤æ–·æ”¶ç›¤å¼·åº¦ï¼ˆ0=æœ€ä½Žé»ž, 1=æœ€é«˜é»žï¼‰\n",
    "    df[\"close_pos\"] = (df[\"Close\"] - df[\"Low\"]) / (price_range + 1e-8)\n",
    "    # å¯¦é«”ã€ä¸Šå½±ç·šã€ä¸‹å½±ç·šæ¯”ä¾‹ï¼šè¾¨è­˜å£“åŠ›èˆ‡æ”¯æ’å€é–“\n",
    "    df[\"body_size\"]    = (df[\"Close\"] - df[\"Open\"]).abs() / (df[\"Open\"] + 1e-8)\n",
    "    df[\"upper_shadow\"] = (df[\"High\"] - df[[\"Open\", \"Close\"]].max(axis=1)) / (df[\"Open\"] + 1e-8)\n",
    "    df[\"lower_shadow\"] = (df[[\"Open\", \"Close\"]].min(axis=1) - df[\"Low\"]) / (df[\"Open\"] + 1e-8)\n",
    "    # æ¼²è·Œåˆ¤æ–·\n",
    "    df[\"is_bullish\"] = (df[\"Close\"] > df[\"Open\"]).astype(int)\n",
    "    df[\"is_bearish\"] = (df[\"Close\"] < df[\"Open\"]).astype(int)\n",
    "    # å¤§é™½ç·š/å¤§é™°ç·šè¾¨è­˜\n",
    "    df[\"marubozu_like\"] = ((df[\"body_size\"] > 0.02) & (df[\"upper_shadow\"] < 0.005) & (df[\"lower_shadow\"] < 0.005)).astype(int)\n",
    "\n",
    "    # ============= 3. å¸ƒæž—é€šé“ (Bollinger Bands) =============\n",
    "    # è¡¡é‡åƒ¹æ ¼æ³¢å‹•ç¯„åœèˆ‡æ¥µç«¯è¶…è²·è³£å€é–“\n",
    "    bb_mid = df[\"Close\"].rolling(20).mean()\n",
    "    bb_std = df[\"Close\"].rolling(20).std()\n",
    "    df[\"BB_mid\"]   = bb_mid\n",
    "    df[\"BB_std\"]   = bb_std\n",
    "    df[\"BB_upper\"] = bb_mid + 2 * bb_std\n",
    "    df[\"BB_lower\"] = bb_mid - 2 * bb_std\n",
    "    # åƒ¹æ ¼åœ¨å¸ƒæž—é€šé“ä¸­çš„ä½ç½®\n",
    "    df[\"BB_pos\"]   = (df[\"Close\"] - df[\"BB_lower\"]) / ((df[\"BB_upper\"] - df[\"BB_lower\"]).replace(0, np.nan) + 1e-8)\n",
    "\n",
    "    # ============= 4. å¤šé€±æœŸå‹•é‡ç‰¹å¾µ =============\n",
    "    # è¡¡é‡ä¸åŒæ™‚é–“æ¡†æž¶ä¸‹çš„å ±é…¬çŽ‡ (3, 5, 10, 20 æ—¥)\n",
    "    df[\"mom3\"]  = df[\"Close\"].pct_change(3)\n",
    "    df[\"mom5\"]  = df[\"Close\"].pct_change(5)\n",
    "    df[\"mom10\"] = df[\"Close\"].pct_change(10)\n",
    "    df[\"mom20\"] = df[\"Close\"].pct_change(20)\n",
    "\n",
    "    # ============= 5. é•·æœŸè¶¨å‹¢å‡ç·šèˆ‡é»ƒé‡‘/æ­»äº¡äº¤å‰ =============\n",
    "    df[\"MA20\"]  = df[\"Close\"].rolling(20).mean()\n",
    "    df[\"MA60\"]  = df[\"Close\"].rolling(60).mean()\n",
    "    df[\"MA120\"] = df[\"Close\"].rolling(120).mean()\n",
    "    df[\"Close_above_MA20\"]  = (df[\"Close\"] > df[\"MA20\"]).astype(int)\n",
    "    df[\"Close_above_MA60\"]  = (df[\"Close\"] > df[\"MA60\"]).astype(int)\n",
    "    df[\"Close_above_MA120\"] = (df[\"Close\"] > df[\"MA120\"]).astype(int)\n",
    "    df[\"MA5_MA20_cross\"]  = (df[\"MA5\"] > df[\"MA20\"]).astype(int)\n",
    "    df[\"MA20_MA60_cross\"] = (df[\"MA20\"] > df[\"MA60\"]).astype(int)\n",
    "\n",
    "    # ============= 6. æˆäº¤é‡é‡èƒ½ç‰¹å¾µ =============\n",
    "    if 'Volume' in df.columns:\n",
    "        # æˆäº¤é‡è®ŠåŒ–çŽ‡èˆ‡é‡èƒ½çˆ†ç™¼è¾¨è­˜\n",
    "        df[\"vol_change\"]   = df[\"Volume\"].pct_change()\n",
    "        vol_ma20           = df[\"Volume\"].rolling(20).mean()\n",
    "        df[\"vol_ratio20\"]  = df[\"Volume\"] / (vol_ma20 + 1e-8)\n",
    "        df[\"vol_spike\"]    = (df[\"vol_ratio20\"] > 1.5).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c5178-8205-4db6-b7fd-ddb999ac96a6",
   "metadata": {},
   "source": [
    "### 3. Feature Set Definition\n",
    "\n",
    "To capture the spillover effects of the U.S. market on the Taiwan stock market, we defined specific feature subsets for model testing:\n",
    "* **US_RAW**: Includes the raw closing prices of the four major U.S. indices (NASDAQ, S&P 500, DJI, and SOX) and TSMC ADR.\n",
    "* **US_CHANGE**: Captures the daily percentage changes of these indices to reflect real-time shifts in market sentiment.\n",
    "* **US_FEATURES**: Integrates the two categories above to construct a comprehensive U.S. market influence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a1c327-49c4-41d2-9fd4-7dd83df1ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_RAW = ['NASDAQ', 'SPX', 'DJI', 'ADR', 'SOX']\n",
    "US_CHANGE = ['NASDAQ_change', 'SPX_change', 'DJI_change']\n",
    "\n",
    "US_FEATURES = US_RAW + US_CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a5fdd-70bb-4f05-9f67-07c2cb8fb6e0",
   "metadata": {},
   "source": [
    "### 4. Smart NA Imputation\n",
    "\n",
    "Due to the extensive use of rolling windows and lags in feature engineering, missing values (NaNs) inevitably occur in the early stages of the dataset. To ensure the integrity of model training, we designed the `fill_na_safely` function to perform categorical imputation:\n",
    "\n",
    "* **Binary Signals**: For features such as moving average crossovers or overbought/oversold signals, missing values are filled with `0`, representing \"no specific event occurred.\"\n",
    "* **Historical Lags**: Missing values are filled with `-1` to distinguish \"absence of historical data\" from \"normal numerical values.\"\n",
    "* **Oscillators and Momentum**: For features such as volatility or Bollinger Band positions, values are filled with `0` to maintain neutral characteristics.\n",
    "* **Continuous Numerical Data**: `ffill` (forward fill) is applied to ensure the logic aligns with time-series reality, strictly avoiding the use of future information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da53739a-732e-4299-9677-bd8ca8fbc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_safely(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½ç¼ºå¤±å€¼å¡«è£œå‡½å¼ (Smart NA Imputation)ï¼š\n",
    "    æ ¹æ“šé‡‘èžç‰¹å¾µçš„ç‰©ç†æ„ç¾©é€²è¡Œåˆ†é¡žè™•ç†ï¼Œé¿å…ç°¡å–®å¡«è£œé€ æˆçš„è³‡è¨Šæ‰­æ›²ã€‚\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        # è·³éŽä¸éœ€å¡«è£œçš„æ¬„ä½ï¼ˆæ™‚é–“ã€æ¨™ç±¤ã€è¼”åŠ©æ¨™ç±¤ï¼‰\n",
    "        if col in ['Date', 'Movement', 'Movement_tomorrow', 'is_train']:\n",
    "            continue\n",
    "\n",
    "        # ============= 1. äºŒå…ƒè¨Šè™Ÿèˆ‡äº‹ä»¶é¡žç‰¹å¾µ =============\n",
    "        # é‚è¼¯ï¼šç¼ºå€¼é€šå¸¸ç™¼ç”Ÿåœ¨è³‡æ–™åˆæœŸï¼ˆé‚„æ²’ç´¯ç©è¶³å¤ å¤©æ•¸ï¼‰ï¼Œæ­¤æ™‚æ‡‰è¦–ç‚ºã€Œç„¡äº‹ä»¶ç™¼ç”Ÿã€\n",
    "        if (\n",
    "            col.endswith('_cross') or      # å‡ç·šæˆ–æŒ‡æ¨™äº¤å‰\n",
    "            col.endswith('_spike') or      # é‡èƒ½æˆ–æ³¢å‹•çˆ†ç™¼\n",
    "            col.endswith('_breakout') or   # åƒ¹æ ¼çªç ´\n",
    "            ('MACD_' in col and 'cross' in col) or\n",
    "            ('K_D_' in col and 'cross' in col) or\n",
    "            col.endswith('_oversold') or   # è¶…è·Œè¨Šè™Ÿ\n",
    "            col.endswith('_overbought') or # è¶…è²·è¨Šè™Ÿ\n",
    "            col.startswith('is_') or       # å¸ƒæž—/Kç·šç‹€æ…‹åˆ¤æ–·\n",
    "            col.endswith('_above_MA5') or\n",
    "            col.endswith('_above_MA10') or\n",
    "            col.endswith('_above_MA20') or\n",
    "            col.endswith('_above_MA60') or\n",
    "            col.endswith('_above_MA120') or\n",
    "            col == 'vol_spike' or\n",
    "            col == 'marubozu_like'\n",
    "        ):\n",
    "            df[col] = df[col].fillna(0) # 0 ä»£è¡¨ã€Œç„¡è¨Šè™Ÿã€\n",
    "            continue\n",
    "\n",
    "        # ============= 2. æ»¯å¾Œé¡žç‰¹å¾µ (Lag Features) =============\n",
    "        # é‚è¼¯ï¼šç•¶è³‡æ–™å°šæœªç´¯ç©åˆ°è¶³å¤ çš„ lag å¤©æ•¸æ™‚ï¼Œä½¿ç”¨ -1 ä½œç‚ºæ¨™è¨˜ã€‚\n",
    "        # é€™èƒ½è®“æ¨¡åž‹ï¼ˆç‰¹åˆ¥æ˜¯ Tree-based æ¨¡åž‹ï¼‰å€åˆ†ã€Œç„¡æ­·å²ã€èˆ‡ã€Œæ•¸å€¼ç‚º 0ã€çš„å·®ç•°ã€‚\n",
    "        if 'lag' in col:\n",
    "            df[col] = df[col].fillna(-1)\n",
    "            continue\n",
    "\n",
    "        # ============= 3. æ»¾å‹•è¨ˆç®—èˆ‡éœ‡ç›ªæŒ‡æ¨™ =============\n",
    "        # é‚è¼¯ï¼šé€™é¡žç‰¹å¾µåæ˜ çš„æ˜¯ç›¸å°ä½ç½®æˆ–å‹•é‡ï¼Œåˆå§‹å€¼è£œ 0 ç¶­æŒä¸­æ€§ã€‚\n",
    "        if (\n",
    "            col.endswith('_5d') or\n",
    "            col.startswith('Volatility') or\n",
    "            col.startswith('BB_') or\n",
    "            col.startswith('mom') or\n",
    "            col in ['close_pos', 'body_size', 'upper_shadow', 'lower_shadow', 'BB_pos']\n",
    "        ):\n",
    "            df[col] = df[col].fillna(0)\n",
    "            continue\n",
    "\n",
    "        # ============= 4. è®Šå‹•çŽ‡èˆ‡å›žå ±çŽ‡é¡žåž‹ =============\n",
    "        # é‚è¼¯ï¼šå°æ–¼è®Šå‹•çŽ‡é¡žç‰¹å¾µï¼ŒNaN é€šå¸¸ç™¼ç”Ÿåœ¨ç¬¬ä¸€ç­†ï¼Œå¡« 0 ä»£è¡¨ã€Œç„¡è®Šå‹•ã€ã€‚\n",
    "        if (\n",
    "            'change' in col.lower() or\n",
    "            'return' in col.lower() or\n",
    "            'ratio' in col.lower()\n",
    "        ):\n",
    "            df[col] = df[col].fillna(0)\n",
    "            continue\n",
    "\n",
    "        # ============= 5. åŸºç¤Žæ•¸å€¼æ¬„ä½ï¼ˆå¦‚è‚¡åƒ¹ã€æŒ‡æ•¸ï¼‰ =============\n",
    "        # é‚è¼¯ï¼šæŽ¡ç”¨æ™‚é–“åºåˆ—æœ€ç©©å¥çš„å¡«è£œæ³•ã€‚\n",
    "        # 1. ffill (å‰å‘å¡«è£œ)ï¼šä½¿ç”¨å‰ä¸€å¤©çš„è³‡æ–™ï¼Œé€™ç¬¦åˆå¯¦éš›äº¤æ˜“é‚è¼¯ï¼ˆä¸åƒè€ƒæœªä¾†è³‡è¨Šï¼‰ã€‚\n",
    "        # 2. bfill (å¾Œå‘å¡«è£œ)ï¼šè‹¥ç¬¬ä¸€ç­†å°±ç¼ºå€¼ï¼Œæ‰å¾€å¾Œæ‰¾ã€‚\n",
    "        # 3. 0ï¼šæœ€å¾Œçš„ä¿åº•å¡«è£œã€‚\n",
    "        if df[col].dtype != 'O':\n",
    "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0cd48-aaeb-4e1a-aacb-206cbd9b9d37",
   "metadata": {},
   "source": [
    "## âš™ï¸ Phase 3: Automated Pipeline & Temporal Alignment\n",
    "\n",
    "To ensure that model training aligns with real-world trading logic and to improve development efficiency, we constructed the `preprocess_one_df` integration function. This phase focuses on addressing common issues in time-series data, such as **Data Leakage** and **Temporal Alignment**.\n",
    "\n",
    "### 1. Target Labeling\n",
    "* **Target Variable**: `Movement_tomorrow`.\n",
    "* **Logic**: We use `.shift(-1)` to move the \"next trading day\" price movement label to the current row. This allows the model to learn how to predict future market directions using features known today.\n",
    "\n",
    "### 2. Market Lag Correction\n",
    "* **Background**: Due to time zone differences, when the Taiwan market opens, the closing information from the U.S. market for the previous night has just been finalized.\n",
    "* **Alignment**: We apply a `.shift(1)` operation to the `US_FEATURES`. This ensures that during the training process, U.S. market features always lag the target label by one day, strictly adhering to backtesting principles that \"only historical information is used to predict the future.\"\n",
    "\n",
    "### 3. Pipeline Automation\n",
    "This function encapsulates the following steps:\n",
    "1. **Temporal Sorting**: Ensures all shift operations are based on the correct chronological order.\n",
    "2. **Feature Engineering Integration**: Calls `enhance_features` to perform multi-dimensional indicator expansion.\n",
    "3. **Missing Value Handling**: Executes `fill_na_safely` for logical data imputation.\n",
    "4. **Invalid Sample Cleaning**: Removes invalid data points at the beginning and end of the dataset caused by `shift` operations (e.g., the last row, which cannot obtain \"tomorrow's\" answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7963293-3cf5-4bfb-8837-9cafaea94d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_df(df_raw: pd.DataFrame, is_train: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å–®ä¸€è³‡æ–™é›†å®Œæ•´é è™•ç†æµæ°´ç·šï¼š\n",
    "    åŒ…å«ï¼šå®šç¾©æ¨™ç±¤ã€è™•ç†æ™‚å·®ã€ç‰¹å¾µå¢žå¼·ã€ç¼ºå¤±å€¼æ¸…ç†ã€‚\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    df['is_train'] = is_train\n",
    "\n",
    "    # 1. æ™‚é–“æŽ’åºï¼šç¢ºä¿æ™‚é–“åºåˆ—çš„é€£çºŒæ€§ï¼Œé€™æ˜¯æ‰€æœ‰ shift æ“ä½œçš„å‰æ\n",
    "    if 'Date' in df.columns:\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # 2. å®šç¾©é æ¸¬ç›®æ¨™ (Target)ï¼š\n",
    "    # ä½¿ç”¨ .shift(-1) å°‡ã€Œæ˜Žå¤©çš„æ¼²è·Œã€ç§»åˆ°ã€Œä»Šå¤©ã€çš„åˆ—ï¼Œè®“æ¨¡åž‹å­¸ç¿’ç”¨ä»Šå¤©é æ¸¬æ˜Žå¤©\n",
    "    df['Movement_tomorrow'] = df['Movement'].shift(-1)\n",
    "\n",
    "    # 3. ç¾Žè‚¡è³‡è¨Šå»¶é²è™•ç† (Market Lag Handling)ï¼š\n",
    "    # ç”±æ–¼ç¾Žè‚¡é–‹ç›¤æ™‚é–“æ™šæ–¼å°è‚¡ï¼Œå°è‚¡ç•¶å¤©é–‹ç›¤æ™‚åªèƒ½åƒè€ƒåˆ°ã€Œç¾Žè‚¡æ˜¨æ™šã€çš„è¡¨ç¾ã€‚\n",
    "    # ä½¿ç”¨ .shift(1) ç¢ºä¿æ¨¡åž‹ä¸æœƒä½¿ç”¨åˆ°ã€Œæœªä¾†ã€çš„ç¾Žè‚¡è³‡è¨Šï¼Œç¬¦åˆçœŸå¯¦äº¤æ˜“é‚è¼¯ã€‚\n",
    "    for col in US_FEATURES:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].shift(1)\n",
    "\n",
    "    # 4. ç‰¹å¾µå·¥ç¨‹å¢žå¼·ï¼šåŸ·è¡Œè‡ªå®šç¾©æŒ‡æ¨™è¨ˆç®— (å¦‚ RSI, MACD, Kç·šåž‹æ…‹ç­‰)\n",
    "    df = enhance_features(df)\n",
    "\n",
    "    # 5. æ™ºèƒ½ NA å¡«è£œï¼šæ ¹æ“šç‰¹å¾µå±¬æ€§é€²è¡Œåˆ†é¡žå¡«è£œï¼Œæœ€å¤§åŒ–æ¨£æœ¬åˆ©ç”¨çŽ‡\n",
    "    df = fill_na_safely(df)\n",
    "\n",
    "    # 6. æ¸…ç†ç„¡æ¨™ç±¤æ¨£æœ¬ï¼š\n",
    "    # ç”±æ–¼ shift(-1)ï¼Œæœ€å¾Œä¸€ç­†è³‡æ–™æœƒæ²’æœ‰ã€Œæ˜Žå¤©çš„ç­”æ¡ˆã€ï¼Œå¿…é ˆå‰”é™¤ä»¥é˜²å¹²æ“¾è¨“ç·´ã€‚\n",
    "    df = df.dropna(subset=['Movement_tomorrow']).reset_index(drop=True)\n",
    "\n",
    "    # 7. åž‹æ…‹è½‰æ›ï¼šç¢ºä¿é æ¸¬ç›®æ¨™ç‚ºåˆ†é¡žæ ¼å¼ (int)\n",
    "    df['Movement_tomorrow'] = df['Movement_tomorrow'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720a5158-c503-419b-9a71-bbc959980b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 4. å° train / test å„è‡ªå‰è™•ç†ï¼ˆé¿å…ä»»ä½•æ´©æ¼ï¼‰\n",
    "# ------------------------------------------------\n",
    "train_proc = preprocess_one_df(train_raw, is_train=1)\n",
    "test_proc  = preprocess_one_df(test_raw,  is_train=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3de7e-9066-4e8b-b10c-38762a51d8c7",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Phase 4: Feature Selection & Cleaning\n",
    "\n",
    "Before model training, we perform a final feature calibration to ensure the quality and stability of the input data.\n",
    "\n",
    "### 1. Automated Feature Filtering\n",
    "* **Exclude Non-Predictive Fields**: Remove `Date` (time-series label), `Movement_tomorrow` (target label), and the auxiliary `is_train` flag.\n",
    "* **Type Filtering**: Automatically iterate through all columns to retain only numerical features, excluding string or object types to ensure compliance with Scikit-learn model input requirements.\n",
    "\n",
    "### 2. Numerical Stability\n",
    "* **Handling Infinite Values**: Financial indicators involving ratios (such as price fluctuations or volume ratios) may occasionally result in `inf` due to zero denominators. These are replaced with `0` to avoid errors during the subsequent scaling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a0dd63-7f8b-4daf-b405-a9f38f129e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 5. å®šç¾©ç‰¹å¾µæ¬„ä½\n",
    "# ------------------------------------------------\n",
    "exclude_cols = ['Date', 'Movement', 'Movement_tomorrow', 'is_train']\n",
    "features = [\n",
    "    c for c in train_proc.columns\n",
    "    if c not in exclude_cols and train_proc[c].dtype != 'O'\n",
    "]\n",
    "\n",
    "X_train_full = train_proc[features]\n",
    "y_train_full = train_proc['Movement_tomorrow']\n",
    "\n",
    "X_test = test_proc[features]\n",
    "y_test = test_proc['Movement_tomorrow']\n",
    "\n",
    "# é¿å… inf\n",
    "X_train_full = X_train_full.replace([np.inf, -np.inf], 0)\n",
    "X_test       = X_test.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ce73b4-07e7-4826-948a-842af57a5c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æª¢æŸ¥æ¨™ç±¤å°é½Š (ä»Šæ—¥ vs æ˜Žæ—¥) ---\n",
      "        Date  Movement  Movement_tomorrow\n",
      "0 2010-01-04         0                  0\n",
      "1 2010-01-05         0                  1\n",
      "2 2010-01-06         1                  0\n",
      "3 2010-01-07         0                  0\n",
      "4 2010-01-08         0                  1\n",
      "\n",
      "--- æª¢æŸ¥ç¾Žè‚¡ç‰¹å¾µå»¶é² (Market Lag) ---\n",
      "åŽŸå§‹ç¬¬äºŒç­† DJI: 10573.68\n",
      "è™•ç†å¾Œç¬¬äºŒç­† DJI (æ‡‰ç‚º NaN æˆ–å·²å¡«è£œ): 10572.02\n",
      "è™•ç†å¾Œç¬¬ä¸‰ç­† DJI (æ‡‰ç­‰æ–¼åŽŸå§‹ç¬¬äºŒç­†): 10573.68\n",
      "\n",
      "--- æª¢æŸ¥æ•¸å€¼å“è³ª ---\n",
      "è¨“ç·´é›†ç¸½æ¨£æœ¬æ•¸: 3680\n",
      "æ˜¯å¦æœ‰éºå¤±å€¼ (NA): False\n",
      "æ˜¯å¦æœ‰ç„¡çª®å¤§ (inf): False\n",
      "ç‰¹å¾µç¸½æ•¸: 117\n"
     ]
    }
   ],
   "source": [
    "# æŒ‘é¸å¹¾å€‹é—œéµæ¬„ä½ä¾†å°é½Šçœ‹çœ‹\n",
    "check_cols = ['Date', 'Movement', 'Movement_tomorrow']\n",
    "print(\"--- æª¢æŸ¥æ¨™ç±¤å°é½Š (ä»Šæ—¥ vs æ˜Žæ—¥) ---\")\n",
    "print(train_proc[check_cols].head(5))\n",
    "\n",
    "print(\"\\n--- æª¢æŸ¥ç¾Žè‚¡ç‰¹å¾µå»¶é² (Market Lag) ---\")\n",
    "# å‡è¨­å¦³åŽŸæœ¬è®€å…¥çš„åŽŸå§‹è³‡æ–™å« train_raw\n",
    "print(\"åŽŸå§‹ç¬¬äºŒç­† DJI:\", train_raw['DJI'].iloc[2])\n",
    "print(\"è™•ç†å¾Œç¬¬äºŒç­† DJI (æ‡‰ç‚º NaN æˆ–å·²å¡«è£œ):\", train_proc['DJI'].iloc[2])\n",
    "print(\"è™•ç†å¾Œç¬¬ä¸‰ç­† DJI (æ‡‰ç­‰æ–¼åŽŸå§‹ç¬¬äºŒç­†):\", train_proc['DJI'].iloc[3])\n",
    "\n",
    "print(\"\\n--- æª¢æŸ¥æ•¸å€¼å“è³ª ---\")\n",
    "print(f\"è¨“ç·´é›†ç¸½æ¨£æœ¬æ•¸: {len(train_proc)}\")\n",
    "print(f\"æ˜¯å¦æœ‰éºå¤±å€¼ (NA): {train_proc[features].isna().any().any()}\")\n",
    "print(f\"æ˜¯å¦æœ‰ç„¡çª®å¤§ (inf): {np.isinf(train_proc[features]).any().any()}\")\n",
    "print(f\"ç‰¹å¾µç¸½æ•¸: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34188923-8998-4d0d-98e9-8387d4b92f08",
   "metadata": {},
   "source": [
    "## âš–ï¸ Phase 5: Time-Series Split\n",
    "\n",
    "In financial forecasting models, the chronological order of samples is critical. To simulate a realistic investment environment, we abandon traditional random sampling in favor of a validation mechanism based on **chronological splitting**.\n",
    "\n",
    "### 1. Splitting Logic\n",
    "* **Training Set (Train)**: Utilizes the first 80% of the historical data (starting from 2010) for model parameter learning.\n",
    "* **Validation Set (Validation)**: Uses the remaining 20% of the training data for hyperparameter fine-tuning (e.g., selecting the optimal K-value for KNN) before the final evaluation.\n",
    "* **Test Set (Test)**: Maintains the final year of data as an independent dataset to serve as the benchmark for final performance evaluation.\n",
    "\n",
    "### 2. No Shuffling Principle\n",
    "This project strictly adheres to the \"No Shuffling\" principle. By ensuring that training data always precedes validation and testing data chronologically, we completely eliminate **Look-ahead Bias**, ensuring that our backtesting results possess genuine practical reference value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7eb908-a17c-4739-8d1f-55429af538d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 2944, Val size = 736, Test size = 205\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# 6. æ™‚é–“åºåˆ—åˆ‡å‰² train / valï¼ˆæŒ‰æ™‚é–“é †åºï¼‰\n",
    "# ------------------------------------------------\n",
    "n_train = len(X_train_full)\n",
    "split_idx = int(n_train * 0.8)\n",
    "\n",
    "X_train = X_train_full.iloc[:split_idx]\n",
    "y_train = y_train_full.iloc[:split_idx]\n",
    "\n",
    "X_val   = X_train_full.iloc[split_idx:]\n",
    "y_val   = y_train_full.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size = {len(X_train)}, Val size = {len(X_val)}, Test size = {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169c9b9-ffa8-4e31-b72c-31d912b618fa",
   "metadata": {},
   "source": [
    "## ðŸ§ª Phase 6: Hyperparameter Tuning - K-Sweep\n",
    "\n",
    "To find the most suitable KNN model parameters for this dataset, we implemented the `sweep_k` function to perform a performance scan across different numbers of neighbors ($k$).\n",
    "\n",
    "### 1. Evaluation Metrics\n",
    "In addition to basic **Accuracy**, we specifically monitor the following metrics:\n",
    "* **Recall (Class 1)**: In stock forecasting, the ability to capture \"Upward\" movements is crucial.\n",
    "* **Macro F1-score**: This accounts for the balance between \"Up\" and \"Down\" classes, ensuring the model performs equally well for both directions without bias.\n",
    "\n",
    "### 2. Scaling Consistency\n",
    "During the sweep process, we strictly execute `fit_transform` on the training set and `transform` on the validation set. This ensures that the model testing environment maintains data scaling consistency identical to future real-world deployment.\n",
    "\n",
    "### 3. Optimization Goal\n",
    "Our objective is to select a $k$ value where both **Macro F1-score** and **Accuracy** are at high levels and the metric performance remains relatively stable, thereby enhancing the model's generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f872a5fb-5521-4e9c-af86-565d6f4b95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_k(X_train, X_val, y_train, y_val, k_list=[3,5,7,11,15,21,31,41]):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"===== Sweep K =====\")\n",
    "    for k in k_list:\n",
    "        knn = KNeighborsClassifier(\n",
    "            n_neighbors=k,\n",
    "            weights='uniform'\n",
    "        )\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        y_pred = knn.predict(X_val_scaled)\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        rec1 = recall_score(y_val, y_pred)\n",
    "        f1m  = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "        print(f\"k={k:2d} | Acc={acc:.4f} | Recall1={rec1:.4f} | MacroF1={f1m:.4f}\")\n",
    "\n",
    "        results.append((k, acc, rec1, f1m))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bb03e-8ea5-47f3-9ec2-da552f6c83dd",
   "metadata": {},
   "source": [
    "### ðŸ“Š Hyperparameter Tuning Result Analysis\n",
    "\n",
    "Following the sweep of $k \\in \\{3, 5, \\dots, 41\\}$, we observed the following:\n",
    "1. **Optimal Parameter**: At $k=11$, the model achieved the highest Accuracy and Macro F1 on the validation set, demonstrating an optimal balance between model complexity and generalization capability.\n",
    "2. **Final Strategy**: We have selected $k=11$ as the definitive parameter for the KNN model to be applied for performance validation on the independent test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318fc209-716b-46a6-8a48-3b4cbecdaf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Sweep K =====\n",
      "k= 3 | Acc=0.4620 | Recall1=0.5050 | MacroF1=0.4581\n",
      "k= 5 | Acc=0.4715 | Recall1=0.5176 | MacroF1=0.4673\n",
      "k= 7 | Acc=0.5041 | Recall1=0.5704 | MacroF1=0.4977\n",
      "k=11 | Acc=0.5408 | Recall1=0.6181 | MacroF1=0.5331\n",
      "k=15 | Acc=0.5204 | Recall1=0.6106 | MacroF1=0.5108\n",
      "k=21 | Acc=0.5054 | Recall1=0.6131 | MacroF1=0.4928\n",
      "k=31 | Acc=0.4946 | Recall1=0.6357 | MacroF1=0.4750\n",
      "k=41 | Acc=0.5122 | Recall1=0.6558 | MacroF1=0.4925\n"
     ]
    }
   ],
   "source": [
    "results = sweep_k(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154b6f19-8602-401c-a85c-3870daed6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_knn(X_train, X_val, y_train, y_val, n_neighbors=11):\n",
    "    # æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # å»ºç«‹æœ€åŸºç¤Žçš„ KNN æ¨¡åž‹\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights='uniform',   # å…ˆç”¨ uniformï¼Œä¹‹å¾Œå¯ä»¥è©¦ distance\n",
    "        metric='minkowski',  # p=2 â†’ Euclidean\n",
    "        p=2\n",
    "    )\n",
    "\n",
    "    # è¨“ç·´\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # é æ¸¬\n",
    "    y_pred = knn.predict(X_val_scaled)\n",
    "\n",
    "    # è©•ä¼°\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    rec_up = recall_score(y_val, y_pred, pos_label=1)\n",
    "    rec_down = recall_score(y_val, y_pred, pos_label=0)\n",
    "    f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    print(\"===== Basic KNN (k = {}) è©•ä¼°çµæžœ =====\".format(n_neighbors))\n",
    "    print(f\"Accuracy        : {acc:.4f}\")\n",
    "    print(f\"Recall (Up=1)   : {rec_up:.4f}\")\n",
    "    print(f\"Recall (Down=0) : {rec_down:.4f}\")\n",
    "    print(f\"Macro F1        : {f1_macro:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "    return knn, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f07da0-2b82-476b-a769-5c136da058ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Basic KNN (k = 11) è©•ä¼°çµæžœ =====\n",
      "Accuracy        : 0.5408\n",
      "Recall (Up=1)   : 0.6181\n",
      "Recall (Down=0) : 0.4497\n",
      "Macro F1        : 0.5331\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.4497    0.4735       338\n",
      "           1     0.5694    0.6181    0.5928       398\n",
      "\n",
      "    accuracy                         0.5408       736\n",
      "   macro avg     0.5347    0.5339    0.5331       736\n",
      "weighted avg     0.5376    0.5408    0.5380       736\n",
      "\n",
      "Confusion Matrix:\n",
      "[[152 186]\n",
      " [152 246]]\n"
     ]
    }
   ],
   "source": [
    "knn_model, knn_scaler = run_basic_knn(X_train, X_val, y_train, y_val, n_neighbors=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2522d93-0b21-4e8d-8916-917fc7f2a19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== KNN Final Test Evaluation =====\n",
      "Accuracy     : 0.4585\n",
      "Recall Up    : 0.4380\n",
      "Recall Down  : 0.4881\n",
      "Macro F1     : 0.4567\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3761    0.4881    0.4249        84\n",
      "           1     0.5521    0.4380    0.4885       121\n",
      "\n",
      "    accuracy                         0.4585       205\n",
      "   macro avg     0.4641    0.4631    0.4567       205\n",
      "weighted avg     0.4800    0.4585    0.4624       205\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41 43]\n",
      " [68 53]]\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = knn_scaler.transform(X_test)\n",
    "y_test_pred = knn_model.predict(X_test_scaled)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "rec_up_test = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "rec_down_test = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "macro_f1_test = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"===== KNN Final Test Evaluation =====\")\n",
    "print(f\"Accuracy     : {acc_test:.4f}\")\n",
    "print(f\"Recall Up    : {rec_up_test:.4f}\")\n",
    "print(f\"Recall Down  : {rec_down_test:.4f}\")\n",
    "print(f\"Macro F1     : {macro_f1_test:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b28422-e9db-4839-ae49-fa48d8687ec6",
   "metadata": {},
   "source": [
    "## ðŸ“‰ Dimensionality Reduction via PCA\n",
    "\n",
    "To enhance model efficiency and reduce high-dimensional noise, we implemented **Principal Component Analysis (PCA)**. This technique transforms the original 117 features into a set of linearly uncorrelated variables.\n",
    "\n",
    "### 1. Motivation for PCA\n",
    "* **Mitigating the Curse of Dimensionality**: With over 100 features, distance-based models like KNN often struggle as the feature space becomes sparse. PCA helps consolidate information into fewer dimensions.\n",
    "* **Noise Reduction**: By retaining only the principal components with the highest variance, we filter out low-variance noise that may interfere with the model's predictive signals.\n",
    "\n",
    "### 2. Implementation Strategy\n",
    "* **Feature Scaling**: Since PCA is sensitive to the scale of the original variables, we apply `StandardScaler` prior to the transformation to ensure each feature contributes equally.\n",
    "* **Component Selection**: We conducted a grid search across different component counts (e.g., 20, 30, 40) to determine the optimal balance between information retention and model performance.\n",
    "* **Result**: Through empirical testing, **40 components** provided the best Macro F1-score, effectively capturing the core variance of the market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c018d488-a0c5-4859-b500-d2a6054af762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def run_pca_knn(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    n_components=30,   # PCA ä¸»æˆåˆ†æ•¸é‡\n",
    "    k_neighbors=11     # KNN çš„ k\n",
    "):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "    # ======= 1. æ¨™æº–åŒ–ï¼ˆPCA & KNN éƒ½éœ€è¦ï¼‰=======\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # ======= 2. PCA é™ç¶­ =======\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca   = pca.transform(X_val_scaled)\n",
    "\n",
    "    # ======= 3. KNN =======\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=k_neighbors,\n",
    "        weights='distance'\n",
    "    )\n",
    "    knn.fit(X_train_pca, y_train)\n",
    "\n",
    "    # ======= 4. é æ¸¬ =======\n",
    "    y_pred = knn.predict(X_val_pca)\n",
    "\n",
    "    # ======= 5. è©•ä¼° =======\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    rec1 = recall_score(y_val, y_pred)\n",
    "    rec0 = recall_score(y_val, y_pred, pos_label=0)\n",
    "    f1m  = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    #print(f\"===== PCA + KNN (PCA={n_components}, k={k_neighbors}) =====\")\n",
    "    #print(f\"Accuracy        : {acc:.4f}\")\n",
    "    #print(f\"Recall (Up=1)   : {rec1:.4f}\")\n",
    "    #print(f\"Recall (Down=0) : {rec0:.4f}\")\n",
    "    #print(f\"Macro F1        : {f1m:.4f}\")\n",
    "    #print(\"\\nClassification Report:\")\n",
    "    #print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'recall_up': rec1,\n",
    "        'recall_down': rec0,\n",
    "        'macro_f1': f1m\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adfc5a78-52e6-4445-9d46-2b546b9bb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_pca_knn(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    pca_list=[10, 20, 30, 40, 50],\n",
    "    k_list=[3, 5, 11, 15, 21, 31, 41, 51]\n",
    "):\n",
    "    best_f1 = -1\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for pca_n in pca_list:\n",
    "        for k in k_list:\n",
    "            result = run_pca_knn(\n",
    "                X_train, X_val, y_train, y_val,\n",
    "                n_components=pca_n,\n",
    "                k_neighbors=k\n",
    "            )\n",
    "\n",
    "            if result[\"macro_f1\"] > best_f1:\n",
    "                best_f1 = result[\"macro_f1\"]\n",
    "                best_params = (pca_n, k)\n",
    "                best_model = result\n",
    "\n",
    "    print(\"\\n===== BEST MODEL =====\")\n",
    "    print(f\"PCA components : {best_params[0]}\")\n",
    "    print(f\"K neighbors    : {best_params[1]}\")\n",
    "    print(f\"Best Macro F1  : {best_f1:.4f}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3353e3b8-26fd-4e81-ae13-ea2aed2c3e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BEST MODEL =====\n",
      "PCA components : 40\n",
      "K neighbors    : 11\n",
      "Best Macro F1  : 0.5129\n"
     ]
    }
   ],
   "source": [
    "best_pca, best_k = search_best_pca_knn(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    pca_list=[20,30,40],\n",
    "    k_list=[5,11,21,31]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f9f911-f1da-4a81-9465-d17431e331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=best_pca)\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a37ef05f-5965-4672-aa2a-ac2b359aa557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== KNN(after PCA) Final Test Evaluation =====\n",
      "Accuracy     : 0.5854\n",
      "Recall Up    : 0.7438\n",
      "Recall Down  : 0.3571\n",
      "Macro F1     : 0.5465\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4918    0.3571    0.4138        84\n",
      "           1     0.6250    0.7438    0.6792       121\n",
      "\n",
      "    accuracy                         0.5854       205\n",
      "   macro avg     0.5584    0.5505    0.5465       205\n",
      "weighted avg     0.5704    0.5854    0.5705       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. scale again\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# 2. PCA again\n",
    "pca = PCA(n_components=best_pca)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca  = pca.transform(X_test_scaled)\n",
    "\n",
    "# 3. Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, weights='distance')\n",
    "knn.fit(X_train_pca, y_train_full)\n",
    "\n",
    "# 4. Predict test\n",
    "y_test_pred = knn.predict(X_test_pca)\n",
    "\n",
    "# 5. Evaluate test\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "rec_up_test = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "rec_down_test = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "macro_f1_test = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"===== KNN(after PCA) Final Test Evaluation =====\")\n",
    "print(f\"Accuracy     : {acc_test:.4f}\")\n",
    "print(f\"Recall Up    : {rec_up_test:.4f}\")\n",
    "print(f\"Recall Down  : {rec_down_test:.4f}\")\n",
    "print(f\"Macro F1     : {macro_f1_test:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df1dfbd8-20b6-4322-be21-f343aa81776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca40 = PCA(n_components=40)\n",
    "X_train_40 = pca40.fit_transform(X_train)\n",
    "X_train_full_40 = pca40.fit_transform(X_train_full)\n",
    "X_val_40   = pca40.transform(X_val)\n",
    "X_test_40  = pca40.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94292c-d1e2-48d7-b1c3-3a9375f845db",
   "metadata": {},
   "source": [
    "## ðŸ§¬ Phase 7: Advanced Feature Engineering - Cluster-Augmented KNN\n",
    "\n",
    "To further capture the non-linear structures and various market states within financial data, we implemented the `run_cluster_knn` function. This represents an innovative approach that combines unsupervised learning with supervised learning.\n",
    "\n",
    "### 1. Core Logic: Spatial Dimension Augmentation\n",
    "* **K-Means Market Clustering**: Following PCA dimensionality reduction, we utilize the K-Means algorithm to cluster the compressed feature space. This effectively identifies specific \"Market Regimes,\" such as volatile periods or trending phases defined by specific combinations of technical indicators.\n",
    "* **Feature Augmentation**: The Cluster Labels generated by K-Means are appended as a new dimension to the feature matrix. This guides the KNN model to prioritize historical samples belonging to the same \"Market State\" when identifying nearest neighbors.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Model Robustness Mechanisms\n",
    "* **Temporal Consistency**: To strictly adhere to backtesting logic, the K-Means model is only \"fitted\" on the training set (`X_train`), while the validation and test sets are processed in \"predict\" mode only.\n",
    "* **Multi-dimensional Metric Evaluation**: Given the inherent difficulty of financial forecasting, we simultaneously monitor **Recall Up** (bullish accuracy) and **Recall Down** (bearish accuracy) to ensure the model maintains an unbiased predictive stance.\n",
    "\n",
    "### 3. Motivation for Optimization\n",
    "Even after PCA reduces the features to 40 dimensions, KNN can still struggle with subtle boundary definitions. By introducing the Cluster Label as a \"logical anchor\" in the distance calculation formula, we provide the model with a clear environmental context, thereby enhancing the resolution of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5f81134-fd40-4aa7-b83f-51e00a7ec9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "def run_cluster_knn(X_train, X_val, y_train, y_val, \n",
    "                    n_clusters=7, n_neighbors=11):\n",
    "\n",
    "    # å…ˆç”¨åŒä¸€å€‹ scaler åš KMeans & KNN\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # KMeans clusteringï¼ˆåª fit åœ¨ trainï¼‰\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_train = kmeans.fit_predict(X_train_scaled)\n",
    "    cluster_val   = kmeans.predict(X_val_scaled)\n",
    "\n",
    "    # å°‡ cluster label åŠ å…¥ç‰¹å¾µ\n",
    "    X_train_aug = np.column_stack([X_train_scaled, cluster_train])\n",
    "    X_val_aug   = np.column_stack([X_val_scaled, cluster_val])\n",
    "\n",
    "    # è¨“ç·´ KNN\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights='uniform',\n",
    "        metric='minkowski',\n",
    "        p=2\n",
    "    )\n",
    "    knn.fit(X_train_aug, y_train)\n",
    "\n",
    "    # é æ¸¬\n",
    "    y_pred = knn.predict(X_val_aug)\n",
    "\n",
    "    # è©•ä¼°\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    rec_up = recall_score(y_val, y_pred, pos_label=1)\n",
    "    rec_down = recall_score(y_val, y_pred, pos_label=0)\n",
    "    macro_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\n===== ClusterKNN: clusters={n_clusters}, k={n_neighbors} =====\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Recall Up    : {rec_up:.4f}\")\n",
    "    print(f\"Recall Down  : {rec_down:.4f}\")\n",
    "    print(f\"Macro F1     : {macro_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": knn,\n",
    "        \"scaler\": scaler,\n",
    "        \"kmeans\": kmeans,\n",
    "        \"val_macro_f1\": macro_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11d806ff-b072-401e-b026-bbbad25daa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ClusterKNN: clusters=3, k=11 =====\n",
      "Accuracy     : 0.5707\n",
      "Recall Up    : 0.7312\n",
      "Recall Down  : 0.3817\n",
      "Macro F1     : 0.5488\n",
      "\n",
      "===== ClusterKNN: clusters=5, k=11 =====\n",
      "Accuracy     : 0.5720\n",
      "Recall Up    : 0.7236\n",
      "Recall Down  : 0.3935\n",
      "Macro F1     : 0.5521\n",
      "\n",
      "===== ClusterKNN: clusters=7, k=11 =====\n",
      "Accuracy     : 0.5774\n",
      "Recall Up    : 0.7387\n",
      "Recall Down  : 0.3876\n",
      "Macro F1     : 0.5557\n",
      "\n",
      "===== ClusterKNN: clusters=10, k=11 =====\n",
      "Accuracy     : 0.5679\n",
      "Recall Up    : 0.7211\n",
      "Recall Down  : 0.3876\n",
      "Macro F1     : 0.5476\n",
      "\n",
      "===== ClusterKNN: clusters=15, k=11 =====\n",
      "Accuracy     : 0.5598\n",
      "Recall Up    : 0.7211\n",
      "Recall Down  : 0.3698\n",
      "Macro F1     : 0.5374\n"
     ]
    }
   ],
   "source": [
    "for c in [3, 5, 7, 10, 15]:\n",
    "    out = run_cluster_knn(\n",
    "        X_train_40, X_val_40, y_train, y_val,\n",
    "        n_clusters=c, \n",
    "        n_neighbors=11\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d3c7ed-4b9f-4d18-86e3-c442d4eee9eb",
   "metadata": {},
   "source": [
    "## ðŸ Phase 8: Final Model Evaluation\n",
    "\n",
    "After completing hyperparameter tuning, we merge the training and validation sets to retrain the optimized **Cluster-Augmented KNN** model using the maximum available historical data. A final performance evaluation is then conducted on the independent test set (2024 data).\n",
    "\n",
    "### 1. Final Model Configuration\n",
    "* **Feature Processing**: Integrates `StandardScaler` normalization with 117-dimensional enhanced features.\n",
    "* **Clustering Strategy**: Employs K-Means clustering with `n_clusters=7` to provide the model with essential market regime context.\n",
    "* **Neighbor Voting**: Configured with `n_neighbors=11` to maintain sensitivity to local features while ensuring robust noise filtration.\n",
    "\n",
    "### 2. Multi-dimensional Evaluation Metrics\n",
    "To comprehensively analyze the model's practical utility, we focus on several key metrics beyond basic **Accuracy**:\n",
    "* **Confusion Matrix**: Provides a direct visualization of the distribution between \"Actual Up/Down\" and \"Predicted Up/Down\" to identify potential model biases.\n",
    "* **Recall Up/Down**: Measures the model's sensitivity in capturing both upward and downward market trends.\n",
    "* **Classification Report**: Provides Precision and F1-score to quantify the overall stability and reliability of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f79322c6-a3bd-443a-afb3-3996b1281b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cluster_knn(X_train_full, y_train_full, X_test, y_test,\n",
    "                      n_clusters=7, n_neighbors=11):\n",
    "    print(\"===== Training Final Cluster-KNN Model =====\")\n",
    "\n",
    "    # StandardScalerï¼ˆfit åœ¨å…¨éƒ¨ trainï¼‰\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # KMeansï¼ˆfit åœ¨å…¨éƒ¨ trainï¼‰\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_train = kmeans.fit_predict(X_train_scaled)\n",
    "    cluster_test  = kmeans.predict(X_test_scaled)\n",
    "\n",
    "    # å°‡ cluster label åŠ å…¥ç‰¹å¾µ\n",
    "    X_train_aug = np.column_stack([X_train_scaled, cluster_train])\n",
    "    X_test_aug  = np.column_stack([X_test_scaled, cluster_test])\n",
    "\n",
    "    # KNN è¨“ç·´\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights='uniform',\n",
    "        metric='minkowski',\n",
    "        p=2\n",
    "    )\n",
    "    knn.fit(X_train_aug, y_train_full)\n",
    "\n",
    "    # Test é æ¸¬\n",
    "    y_pred_test = knn.predict(X_test_aug)\n",
    "\n",
    "    # è©•ä¼°\n",
    "    acc = accuracy_score(y_test, y_pred_test)\n",
    "    rec_up = recall_score(y_test, y_pred_test, pos_label=1)\n",
    "    rec_down = recall_score(y_test, y_pred_test, pos_label=0)\n",
    "    macro_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "    print(\"\\n===== Final Cluster-KNN Test Evaluation =====\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Recall Up    : {rec_up:.4f}\")\n",
    "    print(f\"Recall Down  : {rec_down:.4f}\")\n",
    "    print(f\"Macro F1     : {macro_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test, digits=4))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "    return {\n",
    "        \"knn\": knn,\n",
    "        \"kmeans\": kmeans,\n",
    "        \"scaler\": scaler,\n",
    "        \"macro_f1_test\": macro_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cfef325-2395-40a3-8327-9485d28e70ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Final Cluster-KNN Model =====\n",
      "\n",
      "===== Final Cluster-KNN Test Evaluation =====\n",
      "Accuracy     : 0.6390\n",
      "Recall Up    : 0.5455\n",
      "Recall Down  : 0.7738\n",
      "Macro F1     : 0.6390\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5417    0.7738    0.6373        84\n",
      "           1     0.7765    0.5455    0.6408       121\n",
      "\n",
      "    accuracy                         0.6390       205\n",
      "   macro avg     0.6591    0.6596    0.6390       205\n",
      "weighted avg     0.6803    0.6390    0.6393       205\n",
      "\n",
      "Confusion Matrix:\n",
      "[[65 19]\n",
      " [55 66]]\n"
     ]
    }
   ],
   "source": [
    "result = final_cluster_knn(\n",
    "    X_train_full_40, y_train_full,\n",
    "    X_test_40, y_test,\n",
    "    n_clusters=7,\n",
    "    n_neighbors=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41100653-933b-4d5d-89f3-13b56b9278e0",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Phase 9: Probability Estimation & Decision Support\n",
    "\n",
    "Following the final evaluation, the model is applied to the test set to generate not only binary class predictions but also specific **Upward Probabilities**. This allows us to quantify prediction confidence, serving as a critical filter for real-world trading execution.\n",
    "\n",
    "### 1. Cross-Model Consistency Transformation\n",
    "* **Standardization Application**: The `scaler` generated during the training phase is strictly applied to the test set to ensure consistency across the data space.\n",
    "* **Cluster Mapping**: The pre-trained `K-Means` model is utilized to identify the market regime (Cluster Label) for each sample in the test set.\n",
    "\n",
    "### 2. From \"Classification\" to \"Probability\" (predict_proba)\n",
    "* **Confidence Quantization**: We extract the probability of an \"Upward (Class 1)\" movement using the `predict_proba(X_test_aug)[:, 1]` function.\n",
    "* **Decision Thresholds**: \n",
    "    * A probability near 1.0 indicates a strong bullish consensus among nearest neighbors, representing high confidence.\n",
    "    * A probability near 0.5 suggests a market \"tug-of-war\" between bulls and bears, signaling a need for cautious entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f72c5d4f-c37f-4704-9fe1-31708e78f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç”¨ scaler + kmeans è™•ç† test è³‡æ–™\n",
    "X_test_scaled = result['scaler'].transform(X_test_40)\n",
    "cluster_test = result['kmeans'].predict(X_test_scaled)\n",
    "\n",
    "# 2. å¢žåŠ  cluster label\n",
    "X_test_aug = np.column_stack([X_test_scaled, cluster_test])\n",
    "\n",
    "# 3. ç”¢ç”Ÿä¸Šæ¼²æ©ŸçŽ‡\n",
    "y_pred_prob = result['knn'].predict_proba(X_test_aug)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e69744-08fd-4e69-adaa-67cf9175a356",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Phase 10: Final Output Generation\n",
    "\n",
    "In this phase, the model's prediction results for the 2025 test set are exported into a CSV format. This file serves as the core dataset for subsequent strategy backtesting and live demonstrations.\n",
    "\n",
    "### 1. Output Field Definitions\n",
    "* **Date**: The trading date corresponding to the test set.\n",
    "* **Predict**: Stores the model's predicted \"Upward Probability.\" Compared to binary classification (0/1), probability values provide more granular decision supportâ€”for instance, a probability higher than 0.7 can be treated as a strong buy signal.\n",
    "\n",
    "### 2. Data Encapsulation and Persistence\n",
    "By aligning the prediction results with timestamps, we ensure the connectivity between the model output and original stock price data. This establishes a solid foundation for upcoming model demonstrations and performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe51e53b-9778-463c-a2da-6dfa08f5dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ å·²è¼¸å‡ºï¼š../../../data/results/KNN\\KNN_prediction_2022.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. å®šç¾©è¼¸å‡ºè·¯å¾‘\n",
    "output_dir = \"../../../data/results/KNN\"\n",
    "\n",
    "# 2. å¦‚æžœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå‰‡è‡ªå‹•å»ºç«‹\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"ðŸ“ å·²å»ºç«‹è³‡æ–™å¤¾ï¼š{output_dir}\")\n",
    "\n",
    "# 3. å»ºç«‹è¼¸å‡º DataFrame\n",
    "output_2025 = pd.DataFrame({\n",
    "    \"Date\": test_proc[\"Date\"].values,\n",
    "    \"Predict\": y_pred_prob\n",
    "})\n",
    "\n",
    "# 4. è¼¸å‡ºæˆ CSVï¼ˆæ”¾åˆ°æŒ‡å®šè³‡æ–™å¤¾ä¸‹ï¼‰\n",
    "output_path = os.path.join(output_dir, \"KNN_prediction_2022.csv\")\n",
    "output_2025.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"ðŸ“„ å·²è¼¸å‡ºï¼š{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f4390-4edb-4f2d-9d51-88efb48bf37c",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Phase 11: Dataset Export & Multi-Year Prediction Merging\n",
    "\n",
    "In the final stage of this project, we perform the physical partitioning of data and the automated aggregation of prediction results to ensure that the model output meets the requirements for system deployment.\n",
    "\n",
    "### 1. Temporal Dataset Partitioning\n",
    "* **Training and Test Sets**: Data is strictly partitioned by year (e.g., using 2022 as a boundary) to ensure the validation process adheres to a realistic chronological progression.\n",
    "* **Persistent Storage**: The partitioned files, such as `2022_train.csv` and `2022_test.csv`, are exported to facilitate in-depth backtesting for specific years.\n",
    "\n",
    "\n",
    "\n",
    "> âš ï¸ **Stacking Preparation Reminder**:  \n",
    "> To prevent **Data Leakage**, a \"Rolling Forecast\" approach must be adopted when generating Meta-features for the Stacking ensemble layer:\n",
    "> 1. First, generate physical data slices (train/test) for a specific year (e.g., 2020).\n",
    "> 2. Direct the preprocessing pipeline to that slice, retrain the model, and export `KNN_prediction_2020.csv`.\n",
    "> 3. Repeat this process annually (2020â€“2024), and finally use the `glob` module to perform automated merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5b6eebd3-be62-469c-89db-bc46b0d98083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../data/processed/train.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "903fba72-7f81-4e62-8ed0-7791d0f725fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2020 = df[df[\"Date\"].dt.year < 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "df0683b1-9411-4a10-9f4c-f8173a1d9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2020 = df[df[\"Date\"].dt.year == 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "101f891c-2087-4b2a-b78b-4711f72cdaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜è‡³ï¼š../../data/processed\n"
     ]
    }
   ],
   "source": [
    "# 1. å®šç¾©è™•ç†å¾Œè³‡æ–™çš„å­˜æ”¾è·¯å¾‘\n",
    "processed_dir = \"../../../data/processed\"\n",
    "\n",
    "# 2. æª¢æŸ¥ä¸¦å»ºç«‹è³‡æ–™å¤¾\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "\n",
    "# 3. å®šç¾©å®Œæ•´çš„æª”æ¡ˆè·¯å¾‘\n",
    "train_path_2020 = os.path.join(processed_dir, \"2024_train.csv\")\n",
    "test_path_2020  = os.path.join(processed_dir, \"2024_test.csv\")\n",
    "\n",
    "# 4. å„²å­˜æª”æ¡ˆ\n",
    "df_train_2020.to_csv(train_path_2020, index=False)\n",
    "df_test_2020.to_csv(test_path_2020, index=False)\n",
    "\n",
    "print(f\"âœ… å·²å„²å­˜è‡³ï¼š{processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79d24-7609-4895-997c-38323e54cc7d",
   "metadata": {},
   "source": [
    "### 2. Automated Result Merging\n",
    "* **Batch Retrieval**: Utilizes the `glob` module to automatically detect all files matching the `KNN_prediction_*.csv` pattern, enhancing code scalability.\n",
    "* **Temporal Concatenation**: Yearly prediction probabilities are vertically concatenated and re-sorted by date. This ensures the final `KNN_prediction_all.csv` maintains a complete and continuous time series.\n",
    "* **Final Output**: This consolidated file serves as the core input for the subsequent Stacking ensemble layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "819dc9fc-a477-4ce3-a681-11ba359d760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” å·²è¼¸å‡ºæœ€çµ‚åˆä½µæª”ï¼š../../data/results/KNN\\KNN_prediction_all.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# 1. å®šç¾©è·¯å¾‘\n",
    "input_pattern = \"../../../data/results/KNN/KNN_prediction_*.csv\" # å¾ž results è®€å–å„å¹´ä»½æª”æ¡ˆ\n",
    "output_dir = \"../../../data/results/KNN\"\n",
    "output_file = \"KNN_prediction_all.csv\"\n",
    "\n",
    "# 2. æª¢æŸ¥ä¸¦å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 3. å°‹æ‰¾æ‰€æœ‰é æ¸¬æª”æ¡ˆ\n",
    "files = sorted(glob.glob(input_pattern))\n",
    "\n",
    "if not files:\n",
    "    print(\"âŒ æ‰¾ä¸åˆ°é æ¸¬æª”æ¡ˆï¼Œè«‹ç¢ºèª data/results/ ä¸‹æ˜¯å¦æœ‰æª”æ¡ˆã€‚\")\n",
    "else:\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # åˆä½µæ•¸æ“š\n",
    "    merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # è‹¥æœ‰ Date æ¬„ä½å‰‡é€²è¡ŒæŽ’åº\n",
    "    if \"Date\" in merged.columns:\n",
    "        merged[\"Date\"] = pd.to_datetime(merged[\"Date\"])\n",
    "        merged = merged.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # 4. è¼¸å‡ºåˆ° data/processed/KNN/\n",
    "    final_save_path = os.path.join(output_dir, output_file)\n",
    "    merged.to_csv(final_save_path, index=False)\n",
    "\n",
    "    print(f\"âœ” å·²è¼¸å‡ºæœ€çµ‚åˆä½µæª”ï¼š{final_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
